%% graphmatching.tex - approximate graph matching
%%
%% Copyright 2014, 2015 Jeffrey Finkelstein.
%%
%% This LaTeX markup document is made available under the terms of the Creative
%% Commons Attribution-ShareAlike 4.0 International License,
%% https://creativecommons.org/licenses/by-sa/4.0/.
\documentclass{article}

\usepackage{amsmath}
\usepackage{amssymb}
%% This must come before hyperref.
\usepackage{amsthm}
%% This is strongly recommended by biblatex.
\usepackage[english]{babel}
\usepackage[backend=biber]{biblatex}
\usepackage[T1]{fontenc}
%% This must come before csquotes.
\usepackage[utf8]{inputenc}
\usepackage{lmodern}
%% This is strongly recommended by biblatex.
\usepackage{csquotes}
%% This must come before hyperref.
\usepackage{thmtools}
%% This must come before complexity.
\usepackage{hyperref}
\usepackage{complexity}
\usepackage[firstpage]{draftwatermark}
\usepackage{microtype}
\usepackage{textcomp}

%% Set the amount by which certain characters protrude into the margins.
%%
%% \LoadMicrotypeFile{cmr}
%%
%%     This command forces the built-in protrusion settings for the Computer
%%     Modern Roman (cmr) font family to become available at this point, so
%%     that we can override these settings on the next line. Even though we are
%%     really using the Latin Modern Roman (lmr) fonts, microtype uses the cmr
%%     configuration file.
%%
%% \SetProtrusion
%%
%%     This instructs the microtype package that we are going to modify the
%%     protrusion settings.
%%
%% [load=lmr-T1]
%%
%%     Loads the Type 1 (T1) encoding of the lmr font family, thereby setting
%%     the default protrusion values for all the characters. This is only
%%     possible after the \LoadMicrotypeFile{cmr} command (microtype
%%     essentially considers lmr to be an alias for cmr).
%%
%% {encoding=T1, family=lmr}
%%
%%     Indicates that we are going to modify the protrusion values for the T1
%%     encoding of the lmr font family.
%%
%% \textquotedblright = {,1000} (and similar commands)
%%
%%     Force the character given by \textquotedblright to have default
%%     protrusion on the left margin (given by an empty string before the
%%     comma) and full protrusion (that is, protrusion value 1000) on the right
%%     margin.
\LoadMicrotypeFile{cmr}
\SetProtrusion
    [load=lmr-T1]
    {encoding=T1, family=lmr}
    {
      \textquotedblright = {,1000},
      \textquotedblleft = {1000,},
      {'} = {,1000},
      {,} = {,1000},
      {:} = {,1000},
      {;} = {,1000},
      {.} = {,1000}
    }

%% Set the ``work-in-progress'' watermark for the first page.
\SetWatermarkLightness{0.9}
\SetWatermarkText{Work-in-progress}
\SetWatermarkFontSize{3.5cm}

%% Set the title and author of the PDF file.
\hypersetup{pdftitle={Approximate graph matching}, pdfauthor={Jeffrey Finkelstein}}

%% Declare the bibliography file.
\addbibresource{graphmatching.bib}

%% Declare theorem-like environments.
\declaretheorem[numberwithin=section]{theorem}
\declaretheorem[numberlike=theorem]{corollary}
\declaretheorem[numberlike=theorem, style=definition]{definition}

%% Custom commands are declared here.
\newcommand{\email}[1]{\textlangle\href{mailto:#1}{\nolinkurl{#1}}\textrangle}
\newcommand{\todo}[1]{\textbf{TODO #1}}
\newcommand{\1}{\mathbf{1}}
\newcommand{\MD}{\textsc{Minimum Disagreement}}
\renewcommand{\MA}{\textsc{Maximum Agreement}}
\newcommand{\RMD}{\textsc{Relaxed Minimum Disagreement}}
\newcommand{\RMA}{\textsc{Relaxed Maximum Agreement}}
\newcommand{\MQA}{\textsc{Maximum Quadratic Assignment}}
\newcommand{\MQP}{\textsc{Maximum Quadratic Programming}}
\newcommand{\CQP}{\textsc{Convex Quadratic Programming}}
\newcommand{\SMD}{\textsc{Seeded Minimum Disagreement}}
\newcommand{\SMA}{\textsc{Seeded Maximum Agreement}}
\newcommand{\SRMD}{\textsc{Seeded Relaxed Minimum Disagreement}}
\newcommand{\SRMA}{\textsc{Seeded Relaxed Maximum Agreement}}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\vect}{vec}
\DeclareMathOperator{\agr}{agr}

%% Redefine the footnote environment so it has no reference and no number.
\long\def\symbolfootnote#1{\begingroup%
\def\thefootnote{\fnsymbol{footnote}}\footnotetext{#1}\endgroup}

%% Define the author, title, and date for the document.
\author{Jeffrey~Finkelstein\\ Computer Science Department, Boston University}
\title{Approximate graph matching}

\begin{document}

\maketitle

\symbolfootnote{%
  Copyright 2014, 2015 Jeffrey~Finkelstein \email{jeffreyf@bu.edu}.

  This document is licensed under the Creative Commons Attribution-ShareAlike 4.0 International License, which is available at \mbox{\url{https://creativecommons.org/licenses/by-sa/4.0/}}.
  The \LaTeX{} markup that generated this document can be downloaded from its website at \mbox{\url{https://github.com/jfinkels/graphmatching}}.
  The markup is distributed under the same license.
}

%% Document content goes here.

\section{Notation and definitions}

For any positive integer $n$, let $[n]$ be the set $\{0, \dotsc, n - 1\}$.

\subsection{Matrices}

In this paper, vectors are column vectors.
Unless otherwise specified, the norm of a matrix $\|M\|$ is the Frobenius norm, defined by $\|M\| = \sqrt{\sum_{i, j} M_{ij}^2}$.
%We also use the entrywise $\ell_1$ norm, $\|M\|_1$, defined by $\|M\|_1 = \sum_{i, j} M_{ij}$.
The all ones vector is denoted $\mathbf{1}$ and the all zeros vector is denoted $\mathbf{0}$.
The all ones matrix is denoted $J$ and the all zeros matrix is denoted $O$.
The $n \times n$ identity matrix is denoted $I_n$, or simply $I$ if its size is understood from context.
The vectorization of an $m \times n$ matrix $A$, denoted $\vect(A)$, is the $mn$-dimensional vector created by stacking the rows of $A$ as columns on top of one another.
The trace of a square matrix $A$, denoted $\tr A$, is the sum of its diagonal entries.
The direct sum of square matrices $A$ and $B$, denoted $A \oplus B$, is the block matrix
\begin{equation*}
  \begin{bmatrix}
    A & O \\
    O & B
  \end{bmatrix}.
\end{equation*}
For two matrices $M$ and $N$, entrywise inequality is denoted $M \geq N$.

A nonnegative real matrix $M$ is doubly stochastic if it satisfies the equations $M \mathbf{1} = \mathbf{1}$ and $M^T \mathbf{1} = \mathbf{1}$.
If furthermore each entry in the matrix is either zero or one, the matrix is called a permutation matrix.

\subsection{Graph matching}

Suppose $A$ and $B$ are the adjacency matrices of two undirected graphs on $n$ vertices.
The two graphs are \emph{isomorphic} if there is a permutation matrix $P$ such that $PA = BP$.
The \emph{graph isomorphism problem} is the problem of determining whether such a permutation exists, given $A$ and $B$.
The natural optimization problem corresponding to that decision problem is called the \emph{graph matching problem} and can be expressed as
\begin{align*}
  \text{minimize } & \|PA - BP\|^2 \\
  \text{subject to } & P \1 = \1 \\
  & P^T \1 = \1 \\
  & P \in \{0, 1\}^{n \times n}.
\end{align*}
Since
\begin{align}\label{eq:trace}
  \|PA - BP\|^2 & = \|PA\|^2 + \|BP\|^2 - 2 \langle PA, BP \rangle \nonumber \\
  & = \|PA\|^2 + \|BP\|^2 - 2 \tr(AP^TBP) \nonumber \\
  & = \|A\|^2 + \|B\|^2 - 2 \tr(AP^TBP),
\end{align}
and since $\|A\|$ and $\|B\|$ are constants with respect to the variable $P$, an equivalent formulation of the graph matching problem is
\begin{align*}
  \text{maximize } & \tr(AP^TBP) \\
  \text{subject to } & P \1 = \1 \\
  & P^T \1 = \1 \\
  & P \in \{0, 1\}^{n \times n}.
\end{align*}
A solution to the graph matching problem (in either of these two formulations) is a permutation matrix $P$ that witnesses an isomorphism between the graphs whose adjacency matrices are $A$ and $B$.

Relaxing the constraint ``$P \in \{0, 1\}^{n \times n}$'' to the constraint ``$P \geq O$'', in other words, allowing $P$ to be a doubly stochastic instead of a permutation matrix, yields the \emph{relaxed graph matching problem}.
However, when $P$ is a doubly stochastic matrix, \autoref{eq:trace} does not necessarily hold.
Hence the two formulations of the relaxed graph matching problem corresponding to the two formulations of the graph matching problem are not necessarily equivalent.
For this reason, we give each formulation a different name.

The \emph{convex relaxed graph matching problem} is
\begin{align*}
  \text{minimize } & \|DA - BD\|^2 \\
  \text{subject to } & D \1 = \1 \\
  & D^T \1 = \1 \\
  & D \geq O,
\end{align*}
and the \emph{indefinite relaxed graph matching problem} is
\begin{align*}
  \text{maximize } & \tr(AD^TBD) \\
  \text{subject to } & D \1 = \1 \\
  & D^T \1 = \1 \\
  & D \geq O.
\end{align*}
In either problem, the domain over which to maximize is the set of doubly stochastic matrices, which is the convex hull of the set of permutation matrices.
However, only in the former problem is the objective function convex (hence its name).
\todo{Explain why the objective function is convex.}
The latter problem is called ``indefinite'' because the Hessian matrix of the objective function has trace zero at each point in the domain, and is therefore an indefinite matrix, which indicates that the objective function is \emph{not} convex.\footnote{%
  The \emph{Hessian} of a multivariate function $f$ is the matrix of its second-order partial derivatives; specifically, entry $(i, j)$ in the Hessian matrix is $\frac{\partial^2 f}{\partial x_i \partial x_j}$.
  Evaluating the Hessian matrix entrywise at a point in the domain produces a matrix with real entries.
  If the Hessian matrix evaluated at a critical point is nonsingular, then the signs of the eigenvalues of this matrix indicate whether the critical point is a local minimum, a local maximum, or a saddle point.
  (This is just the multivariate generalization of using the second derivative of a univariate function to determine whether a critical point is a local minimum or a local maximum.)
  If all the eigenvalues are positive (or equivalently, the matrix is positive definite), then the point is a minimum.
  If all the eigenvalues are negative (or equivalently, the matrix is negative definite), then the point is a maximum.
  If some are negative and some are positive (in other words, the matrix is \emph{indefinite}), then the point is a saddle point.
  If a function has a saddle point, then it is not convex.

  If the second-order partial derivatives are locally continuous at the critical point, then the Hessian matrix evaluated at that point is symmetric.
  The trace of a real symmetric matrix equals the sum of its eigenvalues.
  If the sum is zero, some eigenvalues must be positive and some must be negative.
  Therefore, a trace of zero implies a saddle point which implies the function is not convex.%
}
(Computing the Hessian reveals that entry $(ii', jj')$ in the matrix is $A_{ii'} B_{jj'} + A_{i'i} B_{j'j}$ independent of $D$.
Since $A$ and $B$ are symmetric matrices, this is simply $2 A_{ii'} B_{jj'}$.
The trace of the matrix is therefore $2 \tr(A \otimes B)$, a constant function with respect to $D$ whose value is always zero if the graphs being considered do not have self-loops.)

Now \autoref{eq:trace} holds even if $P$ is just a unitary matrix (that is, a matrix satisfying $P^T P = I$), so another relaxation could be
\begin{align*}
  \text{minimize } & \|UA - BU\|^2 \\
  \text{subject to } & U^T U = I \\
  & U \geq O,
\end{align*}
or equivalently,
\begin{align*}
  \text{maximize } & \tr(AU^TBU) \\
  \text{subject to } & U^T U = I \\
  & U \geq O.
\end{align*}
Whereas the relaxed graph matching problems defined above for doubly stochastic matrices have a quadratic objective function and linear constraints, these relaxations have a quadratic objective function with \emph{quadratic} constraints and are therefore more difficult to solve.

\section{Complexity of graph matching problems}

In this section, we have renamed the two formulations of the graph matching problem to \MD{} and \MA{}, and expressed the problems as formal optimization problems.

\todo{Note that formally, optimization problems must have positive measure functions, but this doesn't matter too much.}

\todo{Define budget problem for optimization problems.}

\begin{definition}[\MD]
  \mbox{}

  \begin{tabular}{r p{9.3cm}}
    Instance: & adjacency matrices $A$ and $B$ of two undirected graphs on $n$ vertices. \\
    Solution: & $n \times n$ permutation matrix $P$. \\
    Measure: & $\|PA - BP\|^2$. \\
    Type: & minimization.
  \end{tabular}
\end{definition}

\begin{definition}[\MA]
  \mbox{}

  \begin{tabular}{r p{9.3cm}}
    Instance: & adjacency matrices $A$ and $B$ of two undirected graphs on $n$ vertices. \\
    Solution: & $n \times n$ permutation matrix $P$. \\
    Measure: & $\tr(AP^TBP)$. \\
    Type: & maximization.
  \end{tabular}
\end{definition}

\subsection{Graph matching}

As stated in the previous section, \MD{} and \MA{} are equivalent, so it suffices to study only \MA{}.

\begin{theorem}
  The budget problems for \MD{} and \MA{} are equivalent under polynomial time many-one reductions.
\end{theorem}
\begin{proof}
  As shown in \autoref{eq:trace},
  \begin{equation*}
    \|PA - BP\|^2 = \|A\|^2 + \|B\|^2 - 2 \tr(AP^TBP),
  \end{equation*}
  which is equivalent to
  \begin{equation*}
    \tr(AP^TBP) = \frac{1}{2} (\|A\|^2 + \|B\|^2 - \|PA - BP\|^2).
  \end{equation*}
  By these two equalities,
  \begin{equation*}
    \|PA - BP\|^2 \leq k \iff \tr(AP^TBP) \geq \frac{1}{2} (\|A\|^2 + \|B\|^2 - k)
  \end{equation*}
  and
  \begin{equation*}
    \tr(AP^TBP) \geq k \iff \|PA - BP\|^2 \leq \|A\|^2 + \|B\|^2 - 2k.
  \end{equation*}
  It is straightforward to apply these equivalences to construct the stated reductions.
\end{proof}

These budget problems are $\NP$-complete.
This proof is based on the proof of the $\NP$-completeness of the budget problem for \textsc{Minimum Quadratic Assignment} implicit in \autocite{sg76}.

\begin{theorem}
  The budget problem for \MD{} is $\NP$-complete, as is the budget problem for \MA{}.
\end{theorem}
\begin{proof}
  By the previous theorem, it suffices to show that \MA{} is $\NP$-complete.

  The problem is in $\NP$: the witness is the permutation matrix $P$, and the measure can be computed (and compared to the budget) in polynomial time by matrix multiplication.
  The $\NP$-hardness follows from a polynomial time many-one reduction from the \textsc{Hamiltonian Cycle} problem.
  Given a graph $G$ on $n$ vertices, the reduction is the mapping $G \mapsto (A, B, 2n)$, where $A$ is the adjacency matrix of $G$ and $B$ is the adjacency matrix of the cycle graph on $n$ vertices.
  %%Given a graph $G$ with vertex set $[n]$ and edge set $E$, let the budget be $n$, the matrix $A$ be the adjacency matrix of $G$, and the matrix $B$ be defined as
  %% \begin{equation*}
  %%   B_{ij} =
  %%   \begin{cases}
  %%     1 & \text{if } i + 1 \equiv j \pmod{n} \\
  %%     0 & \text{otherwise}
  %%   \end{cases}
  %% \end{equation*}
  %% for each $i$ and $j$ in $[n]$.
  Each of $A$, $B$, and $2n$ can be computed and written in polynomial time.

  If $G$ has a Hamiltonian cycle, there is some permutation $\pi$ such that the undirected edge $(\pi(i), \pi(i + 1))$ is in $E$ for each $i \in [n]$, where addition is modulo $n$.
  Choosing $P$ to be the permutation matrix corresponding to $\pi$,
  \begin{align*}
    \tr(AP^TBP) & = \langle PA, BP \rangle  \\
    & = \langle PAP^T, B \rangle \\
    & = \sum_{i, j} (PAP^T)_{i, j} B_{i, j} \\
    & = \sum_{i, j} A_{\pi(i), \pi(j)} B_{i, j} \\
    & = 2 \sum_i A_{\pi(i), \pi(i + 1)},
  \end{align*}
  where the last equality is due to the definition of $B$ and the factor of $2$ is due to the fact that both $A$ and $B$ are symmetric matrices.
  Since $A$ is just the adjacency matrix of $G$ and the undirected edge $(\pi(i), \pi(i + 1))$ is in $E$ for each $i$ in $[n]$, the above sum is exactly $2n$.
  In particular, $\tr(AP^TBP) \geq 2n$, so $(A, B, 2n)$ is in the budget problem for \MA{}.

  Now suppose that $\tr(AP^TBP) \geq 2n$.
  The above equalities still hold, so $2\sum_i A_{\pi(i), \pi(i + 1)} \geq 2n$.
  Since the entries of $A$ are in $\{0, 1\}$, this inequality is only satisfied if each $A_{\pi(i), \pi(i + 1)}$ is $1$.
  In other words, for each $i$, the undirected edge $(\pi(i), \pi(i + 1))$ is in $E$, so $\pi$ defines a Hamiltonian cycle in $G$.
  Therefore we have shown a correct polynomial time many-one reduction from \textsc{Hamiltonian Cycle} to \MA{}.
\end{proof}

\textsc{Graph Isomorphism} is the special case of the budget problem for \MD{} in which the budget is zero (in other words, it is the problem of finding a permutation causing no disagreements).
It is also the special case of the budget problem for \MA{} in which the budget is the number of edges in the graphs (in other words, it is the problem of finding a permutation that causes every possible agreement).

\begin{theorem}
  \MD{} and \MA{} are equivalent under \todo{the strictest possible approximation-preserving} reductions.
\end{theorem}

\MA{} is a special case of \MQA{}.

\begin{definition}[\MQA]
  \mbox{}

  \begin{tabular}{r p{9.3cm}}
    Instance: & $n \times n$ real matrices $A$ and $B$. \\
    Solution: & $n \times n$ permutation matrix $P$. \\
    Measure: & $\tr(AP^TBP)$. \\
    Type: & maximization.
  \end{tabular}
\end{definition}

Since the budget problem for \MA{} is $\NP$-complete, so is the budget problem for \MQA{} (I could not find this proven anywhere else).
Since the budget problem is in $\NP$, the optimization problem is in $\NPO$.
Although certain special cases are in $\APX$ (for example, see \autocite{ahs01}), the general problem is $\APX$-hard because some of its special cases are $\APX$-hard \autocite{mms10}.
The general problem has a polynomial time $O(\sqrt{n})$-approximation, but unless $\NP$ has a probabilistic quasipolynomial time simulation, there is no polylogarithmic approximation \autocite{mms10}.

\MA{} is a special case, so it may be more approximable.
\todo{What is the approximability of this problem?}

\subsection{Indefinite relaxed graph matching}

\begin{definition}[\RMA]
  \mbox{}

  \begin{tabular}{r p{9.3cm}}
    Instance: & adjacency matrices $A$ and $B$ of two undirected graphs on $n$ vertices. \\
    Solution: & $n \times n$ doubly stochastic matrix $D$. \\
    Measure: & $\tr(AD^TBD)$. \\
    Type: & maximization.
  \end{tabular}
\end{definition}

\RMA{} is a special case of \MQP{}.

\begin{definition}[\MQP{}]
  \mbox{}

  \begin{tabular}{r p{9.3cm}}
    Instance: & an $m \times n$ real matrix $A$, an $m \times 1$ real vector $b$, an $n \times 1$ real vector $c$, and an $n \times n$ real symmetric matrix $Q$. \\
    Solution: & an $n \times 1$ real vector $x$, subject to the constraints $Ax \leq b$. \\
    Measure: & $\frac{1}{2} x^T Q x + c^T x$. \\
    Type: & maximization.
  \end{tabular}
\end{definition}

Why is the former optimization problem a special case of the latter?
Consider the vector $x$ to be $\vect(D)$.
The feasible area can be defined by the linear constraints $D \1 = \1$ and $D^T \1 = \1$.
By the definition of trace and tensor product, $\tr(AD^TBD) = \vect(D)^T (B \otimes A) \vect(D)$.
For the measure function, choose $Q = 2(B \otimes A)$ and $c = \mathbf{0}$.

The budget problem for \MQP{} is $\NP$-complete \autocite{vavasis90}, so the optimization problem is in $\NPO$.
Although special cases are approximable (for example, \autocite{vavasis92, fly98}), in general no quasipolynomial approximation exists unless $\NP \subseteq \QP$, and no $O(1)$-approximation exists unless $\P = \NP$ \autocite{br95}.

The authors of \autocite{v14} provide what, on first glance, appears to be a fully polynomial time approximation scheme for \RMA{}.
However, they provide no theoretical guarantee on the performance of their approximation algorithm, and it seems unlikely that any such conclusions could be drawn: the objective function is not convex, and hence the approximation guarantees provided by, for example, \autocite[Theorem~2.3]{jaggi11}, do not apply.
\todo{How approximable is this problem?}

Even worse, I believe the objective function is \emph{never} convex, regardless of input graphs.
The quadratic form of the Hessian of the objective function, $x^T H x$, equals $2 \tr(AX \otimes BX)$, where $X$ is the $n \times n$ matrix containing the $n^2$ entries of the vector $x$.
This value is positive if $x$ is a positive vector, so $H$ is not a negative definite matrix, and therefore there are local maxima.

There is a polynomial time constant-factor approximation to the indefinite quadratic programming problem if the matrix of quadratic coefficients $Q$ has a constant number of negative eigenvalues \autocite{vavasis92}.
By the definition of trace, tensor product, and eigenvalues of a matrix, $\lambda_{i,j}(A \otimes B) = \lambda_i(A) \lambda_j(B)$.
Thus if both $A$ and $B$ have a constant number of negative eigenvalues, so does $A \otimes B$.
\todo{Under what conditions on $A$ and $B$ does the corresponding matrix $Q$ have a constant number of negative eigenvalues?}

\subsection{Convex relaxed graph matching}

\begin{definition}[\RMD]
  \mbox{}

  \begin{tabular}{r p{9.3cm}}
    Instance: & adjacency matrices $A$ and $B$ of two undirected graphs on $n$ vertices. \\
    Solution: & $n \times n$ doubly stochastic matrix $D$. \\
    Measure: & $\|DA - BD\|^2$. \\
    Type: & minimization.
  \end{tabular}
\end{definition}

\RMD{} is a special case of the \CQP{} problem (itself a special case of the minimization form of \MQP{}) \todo{reference or explain why this is a special case}.

\begin{definition}[\CQP{}]
  \mbox{}

  \begin{tabular}{r p{9.3cm}}
    Instance: & an $m \times n$ real matrix $A$, an $m \times 1$ real vector $b$, an $n \times 1$ real vector $c$, and an $n \times n$ real symmetric matrix $Q$ such that the measure function is convex in $x$. \\
    Solution: & an $n \times 1$ real vector $x$, subject to the constraints $Ax \leq b$. \\
    Measure: & $\frac{1}{2} x^T Q x + c^T x$. \\
    Type: & minimization.
  \end{tabular}
\end{definition}

\CQP{} is an inherently sequential problem because it admits a polynomial time computable function that produces optimal solutions, but it is a generalization of \textsc{Linear Programming}, which is itself inherently sequential.

\begin{theorem}
  The budget problem for \CQP{} is $\P$-complete.
\end{theorem}
\begin{proof}
  Computing an exact solution to \CQP{} can be done in polynomial time \autocite{gl90}, so the problem is in $\P$.
  Since a linear function is a convex function, and a linear function can be expressed as a quadratic function with $Q = O$, \textsc{Linear Programming} is a special case of \CQP{}, therefore the latter is $\P$-hard since the former is \autocite{ghr95}.
  Therefore \CQP{} is $\P$-complete.
\end{proof}

Since the budget problem for \CQP{} is in $\P$, \RMD{} is in $\PO$.

\todo{
  Is there a highly parallel approximation algorithm for Convex Quadratic Programming?
  For Relaxed Minimum Disagreement?
  Since the objective function is convex, we could use the Frank--Wolfe algorithm to solve this problem, which can be solved in a distributed manner at least (with a sparsity constraint) via \autocite{blgbs15}.
  Can we parallelize Frank--Wolfe by simply choosing a polynomial number of equally spaced starting points in the domain, then recursively shrinking the domain and recursing?
  (This may not work because finding the best direction may require linear programming, which is PO-complete.)}

\subsection{Seeded graph matchings}

\todo{read and interpret \autocite{lfp14, l14, lavpp14}.}

The \emph{seeded graph matching} problem is a generalization of the graph matching problem in which some pairs of vertices are required to be matched.
The optimization problem corresponding to the seeded graph matching problem is called \SMD{}.
We can assume without loss of generality (by applying an additional permutation to the graphs) that if there are $s$ seed pairs, then they are the vertices $\{1, \dotsc, s\}$ in both graphs.
Thus, instead of finding an $n \times n$ permutation matrix $P$ that minimizes $\| PA - BP \|^2$ as in the graph matching problem, the goal is now to find a $(n - s) \times (n - s)$ permutation matrix $P$ that minimizes $\| (I_s \oplus P) A - B (I_s \oplus P) \|^2$.

\begin{definition}[\SMD{}]
  \mbox{}

  \begin{tabular}{r p{9.3cm}}
    Instance: & symmetric $n \times n$ Boolean matrices $A$ and $B$, natural number $s$. \\
    Solution: & $(n - s) \times (n - s)$ permutation matrix $P$. \\
    Measure: & $\| (I_s \oplus P) A - B (I_s \oplus P) \|^2$. \\
    Type: & minimization.
  \end{tabular}
\end{definition}

\begin{definition}[\SMA{}]
  \mbox{}

  \begin{tabular}{r p{9.3cm}}
    Instance: & symmetric $n \times n$ Boolean matrices $A$ and $B$, natural number $s$. \\
    Solution: & $(n - s) \times (n - s)$ permutation matrix $P$. \\
    Measure: & $\tr(A (I_s \oplus P^T) B (I_s \oplus P))$. \\
    Type: & maximization.
  \end{tabular}
\end{definition}

These problems are at least as difficult as their unseeded counterparts, so their budget problems are $\NP$-complete.

\todo{
  How approximable are these problems?
  Can we use an approximation for the unseeded versions to solve the seeded versions?
}

%% \begin{theorem}
%%   \MD{} reduces to \todo{under the strictest approximation-preserving reduction} \SMD{}.
%% \end{theorem}
%% \begin{proof}
%%   The reduction sets the number of seeds to be zero, so a solution to the latter problem is exactly a solution to the former problem.
%% \end{proof}

\subsection{Seeded relaxed graph matchings}

\begin{definition}[\SRMD{}]
  \mbox{}

  \begin{tabular}{r p{9.3cm}}
    Instance: & symmetric $n \times n$ Boolean matrices $A$ and $B$, $s \times s$ doubly stochastic matrix $C$. \\
    Solution: & $(n - s) \times (n - s)$ doubly stochastic matrix $D$. \\
    Measure: & $\| (C \oplus D) A - B (C \oplus D) \|^2$. \\
    Type: & minimization.
  \end{tabular}
\end{definition}

Just as \RMD{} is in $\PO$, so too is \SRMD{}, for the same reason.

\begin{definition}[\SRMA{}]
  \mbox{}

  \begin{tabular}{r p{9.3cm}}
    Instance: & symmetric $n \times n$ Boolean matrices $A$ and $B$, $s \times s$ doubly stochastic matrix $C$. \\
    Solution: & $(n - s) \times (n - s)$ doubly stochastic matrix $D$. \\
    Measure: & $\tr(A (C^T \oplus D^T) B (C \oplus D))$. \\
    Type: & maximization.
  \end{tabular}
\end{definition}

\todo{
  How approximable are these problems?
  Can we use an approximation for the unseeded versions to solve the seeded versions?
}

\section{Using relaxed matchings to recover matchings}

Much of the current research seems to involve computing a relaxed graph matching (using one of the two formulations), then projecting that doubly stochastic matrix onto a permutation matrix in order to get a graph matching.
In \autocite{relax14}, the authors prove that when doing this, it is best to solve the indefinite relaxed graph matching problem instead of the convex relaxed graph matching problem (that is, \RMA{} instead of \RMD{}), because the former is almost always right and the latter almost always wrong; \autocite{v14} follows this strategy.
Still, under certain conditions, solving the convex relaxed graph matching problem can yield the correct permutation matrix \autocite{abk14, fs14}.
\autocite{kmgg14} seems to be the only work simply finding a relaxed graph matching without projecting that doubly stochastic matrix back to a permutation matrix (they are really only finding a relaxed graph matching between a single graph and itself, but it should be possible, using \autocite{gkms14}, to enforce that when running such an algorithm on the disjoint union of two graphs $A$ and $B$, the solution matches some vertices of $A$ with some vertices of $B$).

\section{Amplifying disagreements}

One of our goals is to find a graph operation that, when applied to graphs that are nearly isomorphic, makes the graphs less isomorphic.

Suppose $G$ and $H$ are undirected, unweighted, nonempty graphs on $n$ vertices with adjacency matrices $A$ and $B$, respectively.
Suppose $\pi$ is a bijection from $V(G)$ to $V(H)$ and $P$ is its corresponding permutation matrix.
For each permutation $\pi$, the \emph{agreement between $G$ and $H$ with respect to $\pi$}, denoted $\agr_\pi(G, H)$, is defined by
\begin{equation*}
  \agr_\pi(G, H) = \frac{\tr(AP^TBP)}{\max(\|A\|^2, \|B\|^2)}.
\end{equation*}
In other words, the agreement between $G$ and $H$ is the ratio of the number of edges correctly matched by $\pi$ to the total number of edges (in \autocite{owwz14}, the agreement is defined equivalently in these graph theoretic terms instead of our linear algebraic terms).
The \emph{agreeability between $G$ and $H$}, denoted $\agr(G, H)$, is defined by
\begin{equation*}
  \agr(G, H) = \max_{\pi \in S_n} \agr_\pi(G, H).
\end{equation*}
When $G$ and $H$ admit a better matching, $\agr(G, H)$ increases.
The graphs $G$ and $H$ are isomorphic exactly when $\agr(G, H) = 1$.

Now we hope to find a graph operation $\star$ such that $\agr(G \star X, H \star Y) < \agr(G, H)$ for some graphs $X$ and $Y$.
One initial attempt is the tensor product, denoted $\otimes$.
A second attempt may be graph powering; something more complicated like the zig-zag product might come later.

\begin{definition}[Tensor products]
  \mbox{}
  \begin{itemize}
  \item
    If $A$ and $B$ are $n \times n$ matrices, then the \emph{tensor product of $A$ and $B$}, denoted $A \otimes B$, is an $n^2 \times n^2$ matrix defined as follows.
    Entry $(i, j, i', j')$ of $A \otimes B$ is $a_{i, j} b_{i', j'}$.
  \item If $G$ and $H$ are graphs with adjacency matrices $A$ and $B$, respectively, then the \emph{tensor product of $G$ and $H$}, denoted $G \otimes H$, is the graph whose adjacency matrix is $A \otimes B$.
  \item If $\pi$ and $\tau$ are permutations in $S_n$, then the \emph{tensor product of $\pi$ and $\tau$}, denoted $\pi \otimes \tau$, is a permutation in $S_{n^2}$ defined by $(\pi \otimes \tau)(i, j) = (\pi(i), \tau(j))$.
  \end{itemize}
\end{definition}

\begin{theorem}
  If $\pi$ and $\tau$ are permutations with corresponding permutation matrices $P$ and $Q$, respectively, then $\pi \otimes \tau$ is a permutation with corresponding permutation $P \otimes Q$.
\end{theorem}

\begin{theorem}
  Suppose $A$ and $B$ are $n \times n$ Boolean matrices.
  We have $A = B$ if and only if $A \otimes A = B \otimes B$.
\end{theorem}
\begin{proof}
  If $A = B$ then $A \otimes A = B \otimes B$ immediately from the definitions.
  Suppose $A \neq B$.
  Assume without loss of generality that the top left entry of $A$ is $0$ and the top left entry of $B$ is $1$.
  Then the top left entry of $A \otimes A$ is a $0$ and the top left entry of $B \otimes B$ is $1$.
  Therefore $A \otimes A \neq B \otimes B$.
\end{proof}

\begin{theorem}
  For any undirected, unweighted, nonempty graphs $G$ and $H$ on $n$ vertices, and for any bijection $\pi$ from $V(G)$ to $V(H)$,
  \begin{equation*}
    \agr_{\pi \otimes \pi}(G \otimes G, H \otimes H) = \agr_\pi(G, H)^2.
  \end{equation*}
\end{theorem}
\begin{proof}
  This follows directly from the properties of the tensor product.
  We consider the numerator and the denominator of the agreement separately.
  Suppose $A$ and $B$ are the adjacency matrices of $G$ and $H$, respectively, and suppose $P$ is the permutation matrix corresponding to $\pi$.
  For the numerator,
  \begin{align*}
    \tr((A \otimes A)(P \otimes P)^T(B \otimes B) & (P \otimes P)) \\
    & = \tr((A \otimes A)(P^T \otimes P^T)(B \otimes B)(P \otimes P)) \\
    & = \tr((AP^T \otimes AP^T)(BP \otimes BP)) \\
    & = \tr(AP^TBP \otimes AP^TBP) \\
    & = \tr(AP^TBP)^2.
  \end{align*}
  For the denominator,
  \begin{equation*}
    \max(\|A \otimes A\|^2, \|B \otimes B\|^2) = \max((\|A\|^2)^2, (\|B\|^2)^2) = \max(\|A\|^2, \|B\|^2)^2.
  \end{equation*}
  Combine these two equalities with the definition of the agreement between $G$ and $H$ to produce the equality in the statement of the theorem.
\end{proof}

The next corollary follows from the previous theorem by induction on $\ell$.

\begin{corollary}
  Suppose $\ell$ is a positive integer, $G$ and $H$ are undirected, unweighted, nonempty graphs on $n$ vertices, and $\pi$ is a bijection from $V(G)$ to $V(H)$.

  Let $\mathbf{G} = \bigotimes_{i = 1}^\ell G$, let $\mathbf{H} = \bigotimes_{i = 1}^\ell H$, and let $\Pi = \bigotimes_{i = 1}^\ell \pi$.
  Now
  \begin{equation*}
    \agr_{\Pi}(\mathbf{G}, \mathbf{H}) = \agr_\pi(G, H)^\ell.
  \end{equation*}
\end{corollary}

%% Completeness
This corollary provides a lower bound for the agreement of the larger graphs $\mathbf{G}$ and $\mathbf{H}$ given a lower bound for the agreement of the original graphs $G$ and $H$, though the bound is rather unhelpful because it decreases rapidly.
However, it does show that the tensor product, when used in this way, preserves isomorphism.

%% \begin{theorem}
%%   For any square matrices $P$, $A$, and $B$, if $PA = BP$ then $(P \otimes P)(A \otimes A) = (B \otimes B)(P \otimes P)$.
%% \end{theorem}
%% \begin{proof}
%%   By the mixed-product property of the tensor product,
%%   \begin{equation*}
%%     (P \otimes P)(A \otimes A) = PA \otimes PA = BP \otimes BP = (B \otimes B)(P \otimes P).
%%   \end{equation*}
%% \end{proof}

\begin{theorem}
  Suppose $\ell$ is a positive integer and $G$ and $H$ are undirected, unweighted, nonempty graphs $G$ and $H$ on $n$ vertices.
  Let $\mathbf{G} = \bigotimes_{i = 1}^\ell G$ and $\mathbf{H} = \bigotimes_{i = 1}^\ell H$.
  \begin{equation*}
    \agr(G, H)^\ell \leq \agr(\mathbf{G}, \mathbf{H}),
  \end{equation*}
  with equality when $G$ is isomorphic to $H$.
  In particular, for any $\epsilon \in [0, 1]$, if $\epsilon \leq \agr(G, H)$ then $\epsilon^\ell \leq \agr(\mathbf{G}, \mathbf{H})$.
\end{theorem}
\begin{proof}
  Let $\pi$ be the bijection that maximizes $\agr_\pi(G, H)$ and let $\Pi = \bigotimes_{i = 1}^\ell \pi$.
  By the previous corollary,
  \begin{equation*}
    \agr(G, H)^\ell = \agr_\pi(G, H)^\ell = \agr_\Pi(\mathbf{G}, \mathbf{H}).
  \end{equation*}
  Since $\Pi$ is just one bijection out of all possible bijections between the vertices of $\mathbf{G}$ and $\mathbf{H}$,
  \begin{equation*}
    \agr_\Pi(\mathbf{G}, \mathbf{H}) \leq \max_\tau \agr_\tau(\mathbf{G}, \mathbf{H}) = \agr(\mathbf{G}, \mathbf{H}).
  \end{equation*}
  Combine these equalities and inequalities to produce the inequality in the statement of the theorem.
  If $G$ and $H$ are isomorphic, then $\agr(G, H)^\ell = 1^\ell = 1$, so
  \begin{equation*}
    1 = \agr(G, H)^\ell \leq \agr(\mathbf{G}, \mathbf{H}) \leq 1.
  \end{equation*}

  Finally, if $\epsilon \leq \agr(G, H)$, then $\epsilon^\ell \leq \agr(G, H)^\ell \leq \agr(\mathbf{G}, \mathbf{H})$.
\end{proof}

%% Soundness
What we would like is an upper bound for the agreement of the larger graphs $\mathbf{G}$ and $\mathbf{H}$ given an upper bound for the agreement of the original graphs $G$ and $H$.
Something like if $\agr(G, H) < \epsilon$, then $\agr(\mathbf{G}, \mathbf{H}) < \epsilon' < \epsilon$.
\todo{How to show this?}

\todo{
  Is it true that if $\hat{P}(A \otimes A) = (B \otimes B)\hat{P}$ then $\hat{P} = P \otimes P$ for some permutation matrix $P$?
  If so, then we can conclude that $PA = BP$.
  In \autocite{owwz14}, the authors claim, citing a complicated \oldstylenums{1971} math paper, that if $G \otimes G$ is isomorphic to $H \otimes H$, then $G$ is isomorphic to $H$.
}

\todo{Is it easier to show that some graph product increases disagreement instead of showing that it decreases agreement?}

\todo{What if we use the relaxed versions of the problem here?}

%% Print the bibliography section here.
\printbibliography

\end{document}
